#!/usr/bin/env python3

import argparse
import snakemake 
import os
import sys 
import subprocess
from Bio import SeqIO
from pathlib import Path
import re
from shutil import rmtree
from summarise_snpeff import parse_vcf, write_vcf
from rich.console import Console
from rich.table import Table
from rich.progress import track
from rich.text import Text
from rich import box
import datetime
import pandas as pd
import io 

def get_representative_mutations(anno_file, lineage, cutoff = 0.85, mutation_list_only = False):
    anno_file["nt_aa_compound"] = anno_file.REF + anno_file.POS.astype("str") + anno_file.ALT + "_" + anno_file.residues

    lineage_counts = anno_file.loc[anno_file.lineage == lineage, "sample_id"].nunique()
    mutation_counts = anno_file.loc[anno_file.lineage == lineage].groupby("description").nt_aa_compound.value_counts().rename("count").reset_index()

    if mutation_list_only:
        representative_mutations = mutation_counts.loc[(mutation_counts["count"] >= (lineage_counts * cutoff)), "nt_aa_compound"]
        representative_mutations[["nucleotide_mutation", "residue_mutation"]] = representative_mutations.nt_aa_compound.str.split("_", expand = True)
        representative_mutations.drop(columns = "nt_aa_compound", inplace = True)
    else:
        representative_mutations = mutation_counts.loc[(mutation_counts["count"] >= (lineage_counts * cutoff))].copy()
        representative_mutations[["nucleotide_mutation", "residue_mutation"]] = representative_mutations.nt_aa_compound.str.split("_", expand = True)
        representative_mutations.drop(columns = "nt_aa_compound", inplace = True)
        representative_mutations["sample_count"] = lineage_counts
        representative_mutations["lineage"] = lineage

    return(representative_mutations)

def build_summary_table(input_samples, passing_samples, samples_fail_perc_n, samples_fail_count, cutoff):
    table = Table(show_header=True, header_style="bold magenta", box = box.HORIZONTALS)
    table.add_column("Samples")
    table.add_column("Count")
    table.add_row("Input samples", str(len(input_samples)))
    table.add_row("[green]Passing Samples[/green]", Text(str(len(passing_samples)), "green"))
    table.add_row(f'[red]%N > {str(cutoff)}[/red]', Text(str(samples_fail_perc_n), "red"))
    table.add_row("Incorrect format", str(samples_fail_count))
    return table

def main():

    SPEAR_PATH = os.environ.get('CONDA_PREFIX')
    
    parser = argparse.ArgumentParser('spear')   

    subparsers = parser.add_subparsers(dest='command')
    consensus = subparsers.add_parser('consensus', help='Run SPEAR on consensus FASTA sequence (align first).')
    alignment = subparsers.add_parser('alignment', help='Run SPEAR on alignment in FASTA format (skip alignment).')  
    vcf = subparsers.add_parser('vcf', help='Run SPEAR on existing VCF file(s) - skip alignment and SNP/indel identification and ambiguous SNP filtering.') 
    update = subparsers.add_parser('update', help='Update [spear,data,all]') 
    representative = subparsers.add_parser('representative', help='Obtain representative mutations for a given set of sequences (requires SPEAR annotation file)') 
    report = subparsers.add_parser("report", help = "Generate HTML report from SPEAR output (requires SPEAR annotation file)")
    demo = subparsers.add_parser('demo', help='Run SPEAR demo on lineage VCFs')


    consensus.add_argument('--debug', default = True, action='store_false',
        help="Verbose snakemake execution")
    consensus.add_argument('--dag', default=False, action='store_true',
        help = "Display DAG and exit")
    consensus.add_argument('--no-report', default=False, action='store_true',
        help = "Do not produce HTML report")
    consensus.add_argument('--tmp', default=False, action='store_true',
        help = "Preserve intermediate output files for debugging.")  
    consensus.add_argument('--extension', metavar = '', type = str,
        help = "Suffix and extension for input files")    
    consensus.add_argument('--mask-problem-sites', metavar = 'AB AM HA', nargs='+', 
        help = "Filter problematic sides with these codes: [AB AM HA HH HO IC NA NS NL SS AD BR all]")     
    consensus.add_argument('--threads' , metavar='', type = int, default = 1,
        help = "Max number of threads for snakemake job execution.") 
    consensus.add_argument('--aligner', metavar = '', default = "minimap2", type = str,
        help = "Alignment method to use for alignment to SARS-CoV-2 reference, 'minimap2' or 'muscle', default minimap2")  
    #consensus.add_argument('--allowAmbiguous', default=False, action='store_true',
    #    help = "Toggle whether to exclude ambiguous bases in SNPs and insertions")
    consensus.add_argument('--cutoff', metavar = '', type = int , default=30,
        help = "Percentage N cutoff for input sequences. Default 30")
    consensus.add_argument('--global_n', metavar = '', type = float,
        help = "Minimum percentage of N in sample to flag as poor coverage. Default half of cutoff.")
    consensus.add_argument('--s_n', metavar = '', type = float, default = 0.05,
        help = "Minimum percentage of N in S gene to flag as poor coverage. Default 5.")
    consensus.add_argument('--s_contig', metavar = '', type = float, default = 150,
        help = "Minimum length of contig to flag sample as potential S gene dropout. Default 150nt")
    consensus.add_argument('--rbd_n', metavar = '', type = float, default = 12,
        help = "Number of N's in sample spike RBD to flag as poor. Default 12nt")
    consensus.add_argument('--window' , metavar='', type = int, default = 2,
        help = "Maximum number of flanking N's around deletion, default 2")
    consensus.add_argument('--baseline_scores' , metavar='', type = str,
        help = "Custom baseline scores file for use in summary report")
    consensus.add_argument('--baseline' , metavar='', type = str, default = "BQ.1",
        help = "Baseline sample to use, either from pre-loaded baseline scores or user-supplied custom baseline file. Default BQ.1.")
    consensus.add_argument('--no-product-plot', default=False, action='store_true',
        help = "Do not produce individual sample product plots (for fastest operation)")  
    consensus.add_argument('--pangolin', default="accurate", type = str,
        help = "Pangolin operation mode: accurate (UShER), fast (pangolearn), none (don't run pangolin)")  
    consensus.add_argument('input', metavar='input', type=str,
        help='Input directory of alignments, consensus fasta sequences or VCF files.')
    consensus.add_argument('output', metavar='output', type=str,
        help='Destination dir for SPEAR annotated VCFs')
    consensus.add_argument('--per_sample_outputs', default = False, action= "store_true",
      help ='Specify whether to include updated VCFs and sample level tsv outputs - false = quicker') 

    alignment.add_argument('--debug', default = True, action='store_false',
        help="Verbose snakemake execution")
    alignment.add_argument('--dag', default=False, action='store_true',
        help = "Display DAG and exit")
    alignment.add_argument('--no-report', default=False, action='store_true',
        help = "Do not produce HTML report")     
    alignment.add_argument('--tmp', default=False, action='store_true',
        help = "Preserve intermediate output files for debugging.")        
    alignment.add_argument('--extension', metavar = '', type = str,
        help = "Suffix and extension for input files")
    alignment.add_argument('--mask-problem-sites', metavar = 'AB AM HA', nargs='+', 
        help = "Filter problematic sites with these codes: [AB AM HA HH HO IC NA NS NL SS AD BR all]") 
    alignment.add_argument('--threads' , metavar='', type = int, default = 1,
        help = "Max number of threads for snakemake job execution.")  
    #alignment.add_argument('--allowAmbiguous', default=False, action='store_true',
    #    help = "Toggle whether to exclude ambiguous bases in SNPs and insertions")
    alignment.add_argument('--cutoff', metavar = '', type = int , default=30,
        help = "Percentage N cutoff for input sequences. Default 30")
    alignment.add_argument('--global_n', metavar = '', type = float,
        help = "Minimum percentage of N in sample to flag as poor coverage. Default half of cutoff.")
    alignment.add_argument('--s_n', metavar = '', type = float, default = 0.05,
        help = "Minimum percentage of N in S gene to flag as poor coverage. Default 5.")
    alignment.add_argument('--s_contig', metavar = '', type = float, default = 150,
        help = "Minimum length of contig to flag sample as potential S gene dropout. Default 150nt")
    alignment.add_argument('--rbd_n', metavar = '', type = float, default = 12,
        help = "Number of N's in sample spike RBD to flag as poor. Default 12nt")
    alignment.add_argument('--window' , metavar='', type = int, default = 2,
        help = "Maximum number of flanking N's around deletion, default 2")
    alignment.add_argument('--baseline_scores' , metavar='', type = str,
        help = "Custom baseline scores file for use in summary report")
    alignment.add_argument('--baseline' , metavar='', type = str, default = "BQ.1",
        help = "Baseline sample to use, either from pre-loaded baseline scores or user-supplied custom baseline file. Default BQ.1.")
    alignment.add_argument('--no-product-plot', default=False, action='store_true',
        help = "Do not produce individual sample product plots (for fastest operation)")  
    alignment.add_argument('--pangolin', default="accurate", type = str,
        help = "Pangolin operation mode: accurate (UShER), fast (pangolearn), none (don't run pangolin)")        
    alignment.add_argument('input', metavar='input', type=str,
        help='Input directory of alignments, consensus fasta sequences or VCF files.')
    alignment.add_argument('output', metavar='output', type=str,
        help='Destination dir for SPEAR annotated VCFs')
    alignment.add_argument('--per_sample_outputs', default = False, action= "store_true",
        help ='Specify whether to include updated VCFs and sample level tsv outputs - false = quicker') 

    vcf.add_argument('--debug', default = True, action='store_false',
        help="Verbose snakemake execution")
    vcf.add_argument('--extension', metavar = '', type = str,
        help = "Suffix and extension for input files")
    vcf.add_argument('--tmp', default=False, action='store_true',
        help = "Preserve intermediate output files for debugging.")        
    vcf.add_argument('--dag', default=False, action='store_true',
        help = "Display DAG and exit")
    vcf.add_argument('--no-report', default=False, action='store_true',
        help = "Do not produce HTML report")         
    vcf.add_argument('--mask-problem-sites', metavar = 'AB AM HA', nargs='+', 
        help = "Filter problematic sides with these codes [AB AM HA HH HO IC NA NS NL SS AD BR all]") 
    vcf.add_argument('--threads' , metavar='', type = int, default = 1,
        help = "Max number of threads for snakemake job execution.")
    vcf.add_argument('--baseline_scores' , metavar='', type = str,
        help = "Custom baseline scores file for use in summary report")
    vcf.add_argument('--baseline' , metavar='', type = str, default = "BQ.1",
        help = "Baseline sample to use, either from pre-loaded baseline scores or user-supplied custom baseline file. Default BQ.1.")
    vcf.add_argument('--no-product-plot', default=False, action='store_true',
        help = "Do not produce individual sample product plots (for fastest operation)")
    vcf.add_argument('--pangolin', default="accurate", type = str,
        help = "Pangolin operation mode: accurate (UShER), fast (pangolearn), none (don't run pangolin)")  
    vcf.add_argument('input', metavar='input', type=str,
        help='Input directory of alignments, consensus fasta sequences or VCF files.')
    vcf.add_argument('output', metavar='output', type=str,
        help='Destination dir for SPEAR annotated VCFs')
    vcf.add_argument('--per_sample_outputs', default = False, action= "store_true",
        help ='Specify whether to include updated VCFs and sample level tsv outputs - false = quicker') 
    
    update.add_argument('option', type = str,
        help="Update option : spear, all-data or all")
    
    representative.add_argument('--anno_file', metavar = '', type = str, required = True)
    representative.add_argument('--lineage_file', metavar = '', type = str, required = True, 
                                help = "Lineage file from pangolin lineage assignment or user-defined. Must contain column headers taxon (sample_id in SPEAR annotation) and lineage.")
    representative.add_argument('--lineage', metavar = '', type = str)
    representative.add_argument('--cutoff', metavar = '', type = float, default = 0.85)
    representative.add_argument('--mutation_list_only', default=False, action='store_true')
    representative.add_argument('--output_dir', metavar = '', type = str, default = "representative_mutations",
                                help= "Output file name for representative mutations")
    representative.add_argument('--output_suffix', metavar = '', type = str, default = "representative_mutations",
                                help= "Output file name suffix")

    
    report.add_argument('--no-product-plot', default=False, action='store_true',
        help = "Do not produce individual sample product plots (for fastest operation)")
    report.add_argument('--spear_summary', metavar = '', type = str, required = True,
        help = "SPEAR score summary file")
    report.add_argument('--spear_annotation', metavar = '', type = str, required = True,
        help = "SPEAR annotation file")
    report.add_argument('--baseline_scores' , metavar='', type = str, default = f'{SPEAR_PATH}/data/baseline_scores.tsv',
        help = "Baseline scores file for use in summary report")
    report.add_argument('--input_sample_num' , metavar='', type = int, required=True,
        help = "Number of input samples")
    report.add_argument('--qc_sample_num' , metavar='', type = int, required=True,
        help = "Number of samples passing QC")
    report.add_argument('--n_perc', metavar = '', type = str, default = "spear_score_summary.tsv",
        help = "qc.csv file if produced by SPEAR)")
    report.add_argument('--lineage_report', metavar = '', type = str, default = "lineage_report.csv",
        help= "Lineage report file from pangolin lineage assignment or user-defined. Must contain column headers taxon (sample_id in SPEAR annotation) and lineage. If pangolin not run, pass empty file")
    report.add_argument('--baseline', metavar = '', type = str, default = "BQ.1",
        help = "Baseline sample to use, either from pre-loaded baseline scores or user-supplied custom baseline file. Default BQ.1.")
    report.add_argument('--global_n', metavar = '', type = float,
        help = "Minimum percentage of N in sample to flag as poor coverage. Default half of cutoff.")
    report.add_argument('--s_n', metavar = '', type = float, default = 0.05,
        help = "Minimum percentage of N in S gene to flag as poor coverage. Default 5.")
    report.add_argument('--s_contig', metavar = '', type = float, default = 150,
        help = "Minimum length of contig to flag sample as potential S gene dropout. Default 150nt")
    report.add_argument('--rbd_n', metavar = '', type = float, default = 12,
        help = "Number of N's in sample spike RBD to flag as poor. Default 12nt")
    report.add_argument('--pangolin_command', metavar = '', type = str, default = "pangolin_command.txt",
        help = "File containing pangolin command used to generate lineage report")
    report.add_argument("--output_dir", metavar = '', type = str, default = "report",
        help = "Output directory for report files")
    report.add_argument("--spear_params", metavar = '', type = str, default = "spear_params.txt",
        help = "File containing spear params used to generate original annotations")
    
    args = parser.parse_args()
    start_time = datetime.datetime.now()

    console = Console()
    grid = Table.grid()
    grid.add_column(justify = "center")
    
    
    logo = [
        Text.assemble(("/\\", "bold")), 
        Text.assemble(("/  \\", "bold")),
        Text.assemble(("/ ", "bold"), ("/\\", "bold red"), (" \\", "bold")), 
        Text.assemble(("/_", "bold"), ("/  \\", "bold red"), ("_\\", "bold")), 
        Text.assemble(("/ ", "bold red"), ("/\ ", "bold"), ("\\", "bold red")), 
        Text.assemble(("/_", "bold red"), ("/  \\", "bold"), ("_\\", "bold red")), 
        Text.assemble(("/    \\", "bold")), 
        Text.assemble(("/_    _\\", "bold")),
        Text.assemble(("|", "bold"), ("|", "bold red"),("|", "bold red"), ("|", "bold")), 
        Text.assemble(("|", "bold"), ("|", "bold red"),("|", "bold red"), ("|", "bold")),  
        Text.assemble(("SPEAR: Systematic ProtEin AnnotatoR v2.0.0", "bold red")),
        Text.assemble(("Matt Crown, Matt Bashton 2021-2024", "bold red"))]
    
    for item in logo:
        grid.add_row(item)
    console.print(grid)

    if args.command == "demo":
        args.output = "demo_out"
        args.command = "vcf"
        args.mask_problem_sites = None
        args.no_report = None
        args.baseline_scores = None
        args.baseline = "BQ.1"
        args.no_product_plot = None
        args.debug = True
        args.tmp = False
        args.dag = None
        args.threads = 1
        SPEAR_PATH = os.environ.get('CONDA_PREFIX')
        args.input = f'{SPEAR_PATH}/data/example_vcfs'
        args.extension = ".vcf"
        vcf_in = True
        args.allowAmbiguous = False
        args.window = False
        allow_ambiguous = False
        args.global_n = 1.0
        args.s_n = 1.0
        args.s_contig = 29903
        args.rbd_n = 5000
        args.aligner = None
        args.pangolin = "none"
        args.per_sample_outputs = True

    if args.command == "update":
        subprocess.run(['sh','update_spear.sh',args.option])

    elif args.command == "representative":
        #add some intro text to describe what is happening, sample counts etc.
        Path(f'{args.output_dir}').mkdir(parents=True, exist_ok=True)
        anno_file = pd.read_csv(args.anno_file, sep = '\t')
        lineage_file = pd.read_csv(args.lineage_file)
        lineage_file = lineage_file[["taxon", "lineage"]].copy()
        anno_file = anno_file.merge(lineage_file, left_on = "sample_id", right_on = "taxon", how = "left")
        if anno_file.lineage.isna().any():
            console.print("[red]Warning : some samples do not have a lineage assignment. These will be excluded from representative mutation analysis.[/red]")
            anno_file = anno_file.loc[~anno_file.lineage.isna()].copy()
        lineage_sample_counts = anno_file.groupby("lineage").sample_id.nunique()
        if any(lineage_sample_counts < 5):
            console.print("[red]Warning : some lineages have less than 5 samples. These will be excluded from representative mutation analysis.[/red]")
            anno_file = anno_file.loc[anno_file.lineage.isin(lineage_sample_counts.index[lineage_sample_counts >= 5])].copy()
        if len(anno_file) == 0:
            console.print("[red]Error : no samples found with lineage assignment. Exiting.[/red]")
            sys.exit(1)
        if args.lineage != None:
            mutations = get_representative_mutations(anno_file, args.lineage, cutoff = args.cutoff, mutation_list_only = args.mutation_list_only)
            mutations.to_csv(f'{args.output_dir}/{lineage.replace(".", "_")}_{args.output_suffix}.tsv', sep = '\t', index = False)
        else:
            for lineage in anno_file.lineage.unique():
                mutations = get_representative_mutations(anno_file, lineage, cutoff = args.cutoff, mutation_list_only = args.mutation_list_only)
                mutations.to_csv(f'{args.output_dir}/{lineage.replace(".", "_")}_{args.output_suffix}.tsv', sep = '\t', index = False)

            if args.mutation_list_only:
                mutations.to_csv(f'{args.output_dir}/{lineage.replace(".", "_")}_{args.output_suffix}.tsv', sep = '\t', index = False)
            else:
                mutations.to_csv(f'{args.output_dir}/{lineage.replace(".", "_")}_{args.output_suffix}.tsv', sep = '\t', index = False)
        table = Table(show_header=True, header_style="bold magenta", title = "Test", caption = "test", caption_justify = "center")
        mutations = mutations.applymap(str)
        for column in mutations.columns:
            table.add_column(column)
        for row in mutations.itertuples(index = False):
            table.add_row(*row)
        console.print(table)

    elif args.command == "report":
        images_dir = f'{SPEAR_PATH}/images'
        scripts_dir = f'{SPEAR_PATH}/bin'
        data_dir = f'{SPEAR_PATH}/data'
        #read spear params from file - single line
        with open(args.spear_params, 'r') as f:
            spear_params = f.readline().rstrip('\n')
        
        Path(f'{args.output_dir}').mkdir(parents=True, exist_ok=True)

        #run spear report command in subprocess: 
        subprocess.run(['summary_report.py', '--n_perc', args.n_perc, args.spear_summary, args.spear_annotation, args.baseline_scores, str(args.input_sample_num), str(args.qc_sample_num), images_dir, scripts_dir, data_dir, args.output_dir, args.baseline, str(args.global_n), str(args.s_n), str(args.s_contig), str(args.rbd_n), args.lineage_report, args.pangolin_command, spear_params], check = True)
        

    elif args.command not in ["alignment", "consensus", "vcf", "demo", "representative", "report"]:
        parser.print_help()
        console.print("Please select a subcommand (choose from 'consensus', 'alignment', 'vcf', 'representative', 'update', 'demo', 'report')")
        sys.exit(1)

    else:
        if args.command == "consensus":
            align_in = True
        else:
            align_in = False
            args.aligner = None
        if args.command == "vcf":
            vcf_in = True
            args.allowAmbiguous = False
            args.window = False
            allow_ambiguous = False
            args.global_n = 1.0
            args.s_n = 1.0
            args.s_contig = 29903
            args.rbd_n = 5000
        else:
            vcf_in = False

        SPEAR_PATH = os.environ.get('CONDA_PREFIX')
        args.input = args.input.rstrip("/")
        args.output = args.output.rstrip("/")
        #check if input is a directory or a file and then operate differently depending
        if args.extension == None:
            if args.command == "consensus":
                extension = ".fa"
            elif args.command == "vcf":
                extension = ".vcf"
            else:
                extension = ".aln"
        else:
            extension = args.extension

        passing_samples = []
        Path(f'{args.output}/input_files').mkdir(parents=True, exist_ok=True)
        if args.command == "consensus":
            #input should be a single file, regardless of number of samples. file can be gzipped or not.
            #verify input is a file not a dir
            nperc = subprocess.run(['seqkit', 'fx2tab', f'{args.input}', '-B', 'N', '--name'], stdout=subprocess.PIPE).stdout.decode('utf-8').rstrip('\n')
            input_samples = pd.read_csv(io.StringIO(nperc), sep='\t', names = ["sample_id", "perc_n"])
            samples_fail_perc_n = len(input_samples.loc[input_samples.perc_n >= args.cutoff])
            input_samples["sample_id_clean"] = input_samples["sample_id"].str.replace('[^a-zA-Z0-9.]', '_', regex = True)
            passing_samples = input_samples.loc[input_samples.perc_n < args.cutoff]
            
            passing_samples[["sample_id", "sample_id_clean"]].to_csv(f"{args.output}/input_files/pattern_file.tsv", sep = "\t", header = None, index = False)
            passing_samples[["sample_id"]].to_csv(f"{args.output}/input_files/name_pattern_file.tsv", sep = "\t", header = None, index = False)
            passing_samples = passing_samples.sample_id_clean.values.tolist()
            if len(passing_samples) == 1:
                    single_sample = True
            elif len(passing_samples) > 1:
                    single_sample = False
            else:
                console.print("[red]No samples to process, exiting.[\red]")
                sys.exit(1)
            grep_command = ['seqkit', 'grep', '--quiet', '-f', f'{args.output}/input_files/name_pattern_file.tsv', f'{args.input}']
            replace_command = [
                'seqkit', 'replace', '--quiet',
                '-p', '(.+)$',
                '-k', f'{args.output}/input_files/pattern_file.tsv',
                '-r', '{kv}',
                '-o', f'{args.output}/input_files/input.fasta.gz'
                ]
            
            grep_process = subprocess.Popen(grep_command, stdout=subprocess.PIPE)
            replace_process = subprocess.run(replace_command, stdin=grep_process.stdout, check=True)
            grep_process.stdout.close()
            grep_process.wait()
            samples_fail_count = 0 #only relevent when looking at non concat fa files - where ref may exist
            table = build_summary_table(input_samples, passing_samples, samples_fail_perc_n, samples_fail_count, args.cutoff)

        elif args.command == "vcf":
            header , vcf = parse_vcf(f'{args.input}', split_info_cols = False)
            sample_names_old = vcf.columns[9:]
            if len(sample_names_old) == 0:
                parser.print_help()
                console.print("[red]Error : no samples found[\red]")
                sys.exit(1)
            elif len(sample_names_old) == 1:
                single_sample = True
            else:
                single_sample = False

            sample_names = sample_names_old.str.replace('[^a-zA-Z0-9]', '_')
            input_samples = sample_names.values.tolist()
            colnames = vcf.columns[0:9].values.tolist() + sample_names.values.tolist()
            vcf.columns = colnames
            if vcf["#CHROM"].str.contains("NC_045512\.2|MN908947\.3").all():
                Path(f'{args.output}/input_files').mkdir(parents=True, exist_ok=True)
                vcf["#CHROM"] = "NC_045512.2"
                write_vcf(header, vcf, f'{args.output}/input_files/input.vcf')
                passing_samples = sample_names.values.tolist()
            else:
                samples_fail_chrom_name = sample_names #however with combined file this is all
            if len(passing_samples) == 0:
                console.print("[red]No samples found with matching CHROM field, exiting.[\red]")
                sys.exit(1)      
            samples_fail_perc_n = "Not applicable, VCF input"
            samples_fail_count = "Not applicable, VCF input"
            table = build_summary_table(input_samples, passing_samples, samples_fail_perc_n, samples_fail_count, "NA")
        elif args.command == "alignment":
            if os.path.isfile(args.input):
                single_sample = True
                passing_samples = []
                samples_fail_perc_n = 0
                samples_fail_count = 0
                samples_fail_no_ref = 0
                count = 0
                sample_file = list(SeqIO.parse(f'{args.input}', "fasta"))
                found_ref = False
                ref_regex = re.compile(r'NC_045512\.2|MN908947\.3')
                for record in sample_file:
                    count +=1
                    if ref_regex.search(record.id):
                        record.id = "NC_045512.2"
                        found_ref = True
                        continue
                    elif len(record.seq) == 0:
                        perc_n = 100
                    else:
                        perc_n = (record.seq.count("N")/len(record.seq)) * 100
                if count != 2:
                    samples_fail_count += 1
                elif perc_n >= float(args.cutoff):
                    samples_fail_perc_n += 1
                elif not found_ref:
                    samples_fail_no_ref += 1
                else:
                    sample_file[1].id = re.sub('[^a-zA-Z0-9\.]', '_', sample_file[1].id)
                    sample_file[1].name = re.sub('[^a-zA-Z0-9\.]', '_', sample_file[1].name)
                    sample_file[1].description = re.sub('[^a-zA-Z0-9\.]', '_', sample_file[1].description)
                    with open(f'{args.output}/input_files/{sample_file[1].id}{extension}', "w") as output_handle:
                        SeqIO.write(sample_file, output_handle, "fasta")
                    passing_samples = [sample_file[1].id]
                table = build_summary_table(input_samples, passing_samples, samples_fail_perc_n, samples_fail_count, args.cutoff)
                if len(passing_samples) == 0:
                    print("No samples to process, exiting.")
                    sys.exit(1)
            else:
                single_sample = False
                input_samples = [(f.split(f'{extension}'))[0] for f in os.listdir(args.input) if f.endswith(extension)]
                passing_samples = []
                samples_fail_perc_n = 0
                samples_fail_count = 0
                samples_fail_no_ref = 0
                for sample in input_samples:
                    count = 0
                    sample_file = list(SeqIO.parse(f'{args.input}/{sample}{extension}', "fasta"))
                    found_ref = False
                    ref_regex = re.compile(r'NC_045512\.2|MN908947\.3')
                    for record in sample_file:
                        count +=1
                        if ref_regex.search(record.id):
                            record.id = "NC_045512.2"
                            found_ref = True
                            continue
                        elif len(record.seq) == 0:
                            perc_n = 100
                        else:
                            perc_n = (record.seq.count("N")/len(record.seq)) * 100
                    if count != 2:
                        samples_fail_count += 1
                    elif perc_n >= args.cutoff:
                        samples_fail_perc_n += 1
                    elif not found_ref:
                        samples_fail_no_ref += 1
                    else:
                        sample_file[1].id = re.sub('[^a-zA-Z0-9\.]', '_', sample_file[1].id)
                        sample_file[1].name = re.sub('[^a-zA-Z0-9\.]', '_', sample_file[1].name)
                        sample_file[1].description = re.sub('[^a-zA-Z0-9\.]', '_', sample_file[1].description)
                        with open(f'{args.output}/input_files/{sample_file[1].id}{extension}', "w") as output_handle:
                            SeqIO.write(sample_file, output_handle, "fasta")
                        passing_samples.append(sample_file[1].id)
                table = build_summary_table(input_samples, passing_samples, samples_fail_perc_n, samples_fail_count, args.cutoff)
                if len(passing_samples) == 1:
                    single_sample = True
                elif len(passing_samples) == 0:
                    console.print("[red]No samples to process, exiting.[/red]")
                    sys.exit(1)
        else:
            parser.print_help()
            print("Error : invalid command")
            sys.exit(1)

        # if args.allowAmbiguous:
        #     exclude = ""
        #     allow_ambiguous = "--allowAmbiguous"
        # else:
        exclude = "-ambiguousToN"
        allow_ambiguous = ""
        problem_sites = args.mask_problem_sites

        problem_exc = { 
            "AB" : "ambiguous", 
            "AM" : "amended",
            "HA": "highly_ambiguous", 
            "HH" : "highly_homoplasic", 
            "HO" : "homoplasic", 
            "IC" : "interspecific_contamination", 
            "NA" : "nanopore_adapter", 
            "NS": "narrow_src", 
            "NL": "neighbour_linked", 
            "SS": "single_src", 
            "AD": "amplicon_drop_or_primer_artefact", 
            "BR": "back_to_ref"}

        exclusion_statements = []
        if problem_sites == None:
            filter_snps = False
            filter_statement = ""
        else:
            filter_snps = True
            if "all" in problem_sites:
                filter_statement = f"problem_filter = 'mask'"
            else:
                for site in problem_sites:
                    try:
                        problem_exc[site]
                        statement = f" problem_exc =~ '{problem_exc[site]}' "
                        exclusion_statements.append(statement)

                    except KeyError:
                        print("Masking parameter not recognised")
                        parser.print_help()
                        sys.exit(1)
                exclusions = "|".join(exclusion_statements)
                filter_statement = f"problem_filter = 'mask' & ({exclusions})"

        snakefile = f'{SPEAR_PATH}/bin/pipeline.smk'
        if single_sample: 
            singlesample = "True"
        else:
            singlesample = "False"

        if args.no_report == True:
            report = False
        else:
            report = True

        if args.baseline_scores:
            if os.path.isfile(args.baseline_scores):
                baseline_scores = args.baseline_scores
                baseline_df = pd.read_csv(baseline_scores, sep = '\t')
            else:
                console.print("Error, user specified baseline scores file not found. Exiting.")
                sys.exit(1)
        else:
            baseline_scores = f'{SPEAR_PATH}/data/baseline_scores.tsv'
            baseline_df = pd.read_csv(baseline_scores, sep = '\t')

        scores_cols = [
            "sample_id",	
            "total_variants",
            "total_residue_variants",
            "consequence_type_variants",
            "region_residues",
            "domain_residues",
            "feature_residues",
            "ACE2_contact_counts",
            "ACE2_contact_score",
            "trimer_contact_counts",
            "trimer_contact_score",
            "barnes_class_variants",
            "bloom_ACE2_wuhan_mean",
            "bloom_ACE2_wuhan_max",
            "bloom_ACE2_wuhan_min",
            "bloom_ACE2_BA1_mean",
            "bloom_ACE2_BA1_max",
            "bloom_ACE2_BA1_min",
            "bloom_ACE2_BA2_mean",
            "bloom_ACE2_BA2_max",
            "bloom_ACE2_BA2_min",
            "VDS_sum",
            "VDS_max",
            "VDS_min",
            "serum_escape_sum",
            "serum_escape_max",
            "serum_escape_min",
            "mAb_escape_all_classes_sum",
            "mAb_escape_all_classes_max",
            "mAb_escape_all_classes_min",
            "cm_mAb_escape_all_classes_sum",
            "cm_mAb_escape_all_classes_max",
            "cm_mAb_escape_all_classes_min",
            "mAb_escape_class_1_sum",
            "mAb_escape_class_1_max",
            "mAb_escape_class_1_min",
            "mAb_escape_class_2_sum",
            "mAb_escape_class_2_max",
            "mAb_escape_class_2_min",
            "mAb_escape_class_3_sum",
            "mAb_escape_class_3_max",
            "mAb_escape_class_3_min",
            "mAb_escape_class_4_sum",
            "mAb_escape_class_4_max",
            "mAb_escape_class_4_min",
            "BEC_EF_sample"]

        if baseline_df.columns.tolist() != scores_cols:
            console.print("Error, baseline scores file format incorrect, are you missing a field from spear_scores_summary.tsv ? Exiting.")
            sys.exit(1)

        if baseline_df["sample_id"].isin([args.baseline]).any():
            baseline = args.baseline
        else:
            console.print("Error, user specified baseline sample not found in baseline file. Exiting.")
            sys.exit(1)
        
        if single_sample:
            input_sample_num = 1
        else:
            input_sample_num = len(input_samples)
        qc_sample_num = len(passing_samples)

        if args.no_product_plot:
            product_plot = ""
        else:
            product_plot = "--product_plots"

        if args.command in ["consensus", "alignment"]:
            if not args.global_n:
                args.global_n = (args.cutoff/100) * 0.5
        
        if args.command == "vcf":
            cutoff = 0.3
        else:
            cutoff = args.cutoff/100

        if args.per_sample_outputs == True:
            per_sample_outputs = "True"
        else:
            if single_sample:
                per_sample_outputs = "True"
            else:
                per_sample_outputs = "False"


        spear_params = f'spear:{args.command},mask-problem-sites:{problem_sites},aligner:{args.aligner},cutoff:{cutoff},global_n:{args.global_n},s_n:{args.s_n},s_contig:{args.s_contig},rbd_n:{args.rbd_n},window:{args.window}'
        with open(f'{args.output}/spear_params.txt', 'w') as f:
            f.write(spear_params)
        
        args.s_contig = int(args.s_contig)
        config = {
            "samples" : passing_samples, 
            "output_dir" : args.output,
            "data_dir" : f'{SPEAR_PATH}/data',
            "align" : align_in,
            "aligner" : args.aligner,
            "exclude_ambiguous" : exclude,
            "reference_sequence" : f'{SPEAR_PATH}/data/reference.fasta',
            "filter" : filter_snps,
            "filter_params" : filter_statement,
            "del_window" : args.window,
            "extension" : extension,
            "vcf" : vcf_in,
            "single_sample" : single_sample,
            "allow_ambiguous" : allow_ambiguous,
            "report": report,
            "images_dir" : f'{SPEAR_PATH}/images',
            "scripts_dir" : f'{SPEAR_PATH}/bin',
            "baseline" : baseline,
            "baseline_scores": baseline_scores,
            "input_sample_num" : input_sample_num,
            "qc_sample_num" : qc_sample_num,
            "product_plots" : product_plot,
            "global_n" : args.global_n,
            "s_n" : args.s_n , 
            "s_contig" : args.s_contig,
            "rbd_n" : args.rbd_n,
            "pangolin" : args.pangolin,
            "threads" : args.threads,
            "max_n" : cutoff, 
            "spear_params" : spear_params,
            "per_sample_outputs" : per_sample_outputs,
            "single_sample" : singlesample}
        
        console.print(table)

        status = snakemake.snakemake(
                    snakefile, 
                    printshellcmds=False,
                    config=config, 
                    quiet= args.debug,
                    forceall = False, 
                    printdag = args.dag, 
                    cores = args.threads,
                    printreason = False)

        if status:
            end_time = datetime.datetime.now()
            c = end_time - start_time
            minutes = int(c.total_seconds() // 60)
            seconds = c.total_seconds() % 60
            
            console.print(f'Analysis complete! {str(len(passing_samples))} samples analysed in {str(minutes)} mins {str(round(seconds,2))} secs. :white_check_mark:')
            
            c = end_time - start_time

            if not args.tmp and not args.dag:
                rmtree(f'{args.output}/intermediate_output/')
                rmtree(f'{args.output}/input_files/')
            return 0
        return 1


if __name__ == "__main__":
    main()